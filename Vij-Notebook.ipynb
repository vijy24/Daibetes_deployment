{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.get(name=\"vij-workspace-1\",\n",
        "                   subscription_id=\"1a1d0c91-b4c7-49a1-a6ef-2cea4ddaadb3\",\n",
        "                   resource_group=\"vij-resource-group-1\")\n",
        "\n",
        "print(\"Connected to Azure ML Workspace:\", ws.name)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Connected to Azure ML Workspace: vij-workspace-1\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1747400428370
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "import pandas as pd\n",
        "\n",
        "diabetes = load_diabetes()\n",
        "data = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
        "data['target'] = diabetes.target\n",
        "data = data.head(100)  # Curtail to 100 rows\n",
        "\n",
        "data.to_csv(\"diabetes.csv\", index=False)\n",
        "print(\"Dataset saved as diabetes.csv\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset saved as diabetes.csv\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1747400431570
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore, Dataset\n",
        "datastore = ws.get_default_datastore()\n",
        "datastore.upload_files([\"diabetes.csv\"], target_path=\"diabetes-data\", overwrite=True)\n",
        "dataset = Dataset.Tabular.from_delimited_files(path=(datastore, \"diabetes-data/diabetes.csv\"))\n",
        "print(\"Dataset uploaded to Azure ML\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 1 files\nUploading diabetes.csv\nUploaded diabetes.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nDataset uploaded to Azure ML\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1747400440175
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "\n",
        "compute_name = \"Vij-Compute\"\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "else:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_E4DS_V4\", max_nodes=4)\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, compute_config)\n",
        "    compute_target.wait_for_completion(show_output=True)\n",
        "\n",
        "print(\"Compute Target Ready:\", compute_target.name)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Compute Target Ready: Vij-Compute\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1747400447748
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
        "experiment = Experiment(workspace=ws, name=\"diabetes-experiment\")\n",
        "env = Environment(name=\"diabetes-env\")\n",
        "env.python.conda_dependencies.add_pip_package(\"scikit-learn\")\n",
        "env.python.conda_dependencies.add_pip_package(\"pandas\")\n",
        "env.python.conda_dependencies.add_pip_package(\"joblib\")\n",
        "src = ScriptRunConfig(source_directory=\".\",\n",
        "                      script=\"training.py\",\n",
        "                      compute_target=compute_target,\n",
        "                      environment=env)\n",
        "run = experiment.submit(src)\n",
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: diabetes-experiment_1747400453_7cf23727\nWeb View: https://ml.azure.com/runs/diabetes-experiment_1747400453_7cf23727?wsid=/subscriptions/1a1d0c91-b4c7-49a1-a6ef-2cea4ddaadb3/resourcegroups/vij-resource-group-1/workspaces/vij-workspace-1&tid=d25ccddb-b863-4a84-8923-dbc1177436a0\n\nStreaming user_logs/std_log.txt\n===============================\n\nPython executable: /azureml-envs/azureml_e5f5c1ae3d2957fcce6534e509fda810/bin/python\nPython version: 3.8.13 (default, Oct 21 2022, 23:50:54) \n[GCC 11.2.0]\nPython packages path: ['/mnt/azureml/cr/j/f960e4b41b0d4ca2b141b81053fba552/exe/wd', '', '/azureml-envs/azureml_e5f5c1ae3d2957fcce6534e509fda810/lib/python38.zip', '/azureml-envs/azureml_e5f5c1ae3d2957fcce6534e509fda810/lib/python3.8', '/azureml-envs/azureml_e5f5c1ae3d2957fcce6534e509fda810/lib/python3.8/lib-dynload', '/azureml-envs/azureml_e5f5c1ae3d2957fcce6534e509fda810/lib/python3.8/site-packages', '/mnt/azureml/cr/j/f960e4b41b0d4ca2b141b81053fba552/exe/wd', '/azureml-envs/azureml_e5f5c1ae3d2957fcce6534e509fda810/lib/python3.8/site-packages/setuptools/_vendor', '/azureml-envs/azureml_e5f5c1ae3d2957fcce6534e509fda810/lib/python3.8/site-packages/azureml/_project/vendor']\nCleaning up all outstanding Run operations, waiting 300.0 seconds\n1 items cleaning up...\nCleanup took 5.359349727630615 seconds\n\nExecution Summary\n=================\nRunId: diabetes-experiment_1747400453_7cf23727\nWeb View: https://ml.azure.com/runs/diabetes-experiment_1747400453_7cf23727?wsid=/subscriptions/1a1d0c91-b4c7-49a1-a6ef-2cea4ddaadb3/resourcegroups/vij-resource-group-1/workspaces/vij-workspace-1&tid=d25ccddb-b863-4a84-8923-dbc1177436a0\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "{'runId': 'diabetes-experiment_1747400453_7cf23727',\n 'target': 'Vij-Compute',\n 'status': 'Completed',\n 'startTimeUtc': '2025-05-16T13:01:01.593877Z',\n 'endTimeUtc': '2025-05-16T13:01:16.391773Z',\n 'services': {},\n 'properties': {'_azureml.ComputeTargetType': 'amlcdsi',\n  '_azureml.ClusterName': 'Vij-Compute',\n  'ContentSnapshotId': '6b39ef9a-a519-424d-a2b3-d8eb3961faac',\n  'ProcessInfoFile': 'azureml-logs/process_info.json',\n  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n 'inputDatasets': [],\n 'outputDatasets': [],\n 'runDefinition': {'script': 'training.py',\n  'command': '',\n  'useAbsolutePath': False,\n  'arguments': [],\n  'sourceDirectoryDataStore': None,\n  'framework': 'Python',\n  'communicator': 'None',\n  'target': 'Vij-Compute',\n  'dataReferences': {},\n  'data': {},\n  'outputData': {},\n  'datacaches': [],\n  'jobName': None,\n  'maxRunDurationSeconds': 2592000,\n  'nodeCount': 1,\n  'instanceTypes': [],\n  'priority': None,\n  'credentialPassthrough': False,\n  'identity': None,\n  'environment': {'name': 'diabetes-env',\n   'version': 'Autosave_2025-05-14T13:37:44Z_bc7ba4cb',\n   'assetId': 'azureml://locations/uksouth/workspaces/1d8877c1-1f8d-42bb-b522-213ed87bcfdd/environments/diabetes-env/versions/Autosave_2025-05-14T13:37:44Z_bc7ba4cb',\n   'autoRebuild': True,\n   'python': {'interpreterPath': 'python',\n    'userManagedDependencies': False,\n    'condaDependencies': {'name': 'project_environment',\n     'dependencies': ['python=3.8.13',\n      {'pip': ['azureml-defaults', 'scikit-learn', 'pandas', 'joblib']}],\n     'channels': ['anaconda', 'conda-forge']},\n    'baseCondaEnvironment': None},\n   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04',\n    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n    'baseDockerfile': None,\n    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n    'enabled': False,\n    'arguments': []},\n   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n   'inferencingStackVersion': None},\n  'history': {'outputCollection': True,\n   'directoriesToWatch': ['logs'],\n   'enableMLflowTracking': True,\n   'snapshotProject': True},\n  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n    'spark.yarn.maxAppAttempts': '1'}},\n  'parallelTask': {'maxRetriesPerWorker': 0,\n   'workerCountPerNode': 1,\n   'terminalExitCodes': None,\n   'configuration': {}},\n  'amlCompute': {'name': None,\n   'vmSize': None,\n   'retainCluster': False,\n   'clusterMaxNodeCount': None},\n  'aiSuperComputer': {'instanceType': 'D2',\n   'imageVersion': None,\n   'location': None,\n   'aiSuperComputerStorageData': None,\n   'interactive': False,\n   'scalePolicy': None,\n   'virtualClusterArmId': None,\n   'tensorboardLogDirectory': None,\n   'sshPublicKey': None,\n   'sshPublicKeys': None,\n   'enableAzmlInt': True,\n   'priority': 'Medium',\n   'slaTier': 'Standard',\n   'userAlias': None},\n  'kubernetesCompute': {'instanceType': None},\n  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n  'mpi': {'processCountPerNode': 1},\n  'pyTorch': {'communicationBackend': 'nccl', 'processCount': None},\n  'hdi': {'yarnDeployMode': 'Cluster'},\n  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n  'exposedPorts': None,\n  'docker': {'useDocker': False,\n   'sharedVolumes': True,\n   'shmSize': '2g',\n   'arguments': []},\n  'cmk8sCompute': {'configuration': {}},\n  'commandReturnCodeConfig': {'returnCode': 'Zero',\n   'successfulReturnCodes': []},\n  'environmentVariables': {},\n  'applicationEndpoints': {},\n  'parameters': []},\n 'logFiles': {'user_logs/std_log.txt': 'https://vijworkspace17376385887.blob.core.windows.net/azureml/ExperimentRun/dcid.diabetes-experiment_1747400453_7cf23727/user_logs/std_log.txt?sv=2019-07-07&sr=b&sig=6lBoXgpxOKB6hdwC0OlqBnLJxuJOnB4fBefSWL1jdQ4%3D&skoid=24a8141d-fcb3-4745-b989-08fa03cec8d2&sktid=d25ccddb-b863-4a84-8923-dbc1177436a0&skt=2025-05-16T00%3A57%3A35Z&ske=2025-05-18T01%3A07%3A35Z&sks=b&skv=2019-07-07&st=2025-05-16T12%3A51%3A19Z&se=2025-05-16T21%3A01%3A19Z&sp=r',\n  'system_logs/cs_capability/cs-capability.log': 'https://vijworkspace17376385887.blob.core.windows.net/azureml/ExperimentRun/dcid.diabetes-experiment_1747400453_7cf23727/system_logs/cs_capability/cs-capability.log?sv=2019-07-07&sr=b&sig=e0t7MFgsXuVOuXLOSJDOZah3U0UklrHk%2BNZjFoWgESU%3D&skoid=24a8141d-fcb3-4745-b989-08fa03cec8d2&sktid=d25ccddb-b863-4a84-8923-dbc1177436a0&skt=2025-05-16T00%3A57%3A35Z&ske=2025-05-18T01%3A07%3A35Z&sks=b&skv=2019-07-07&st=2025-05-16T12%3A51%3A19Z&se=2025-05-16T21%3A01%3A19Z&sp=r',\n  'system_logs/hosttools_capability/hosttools-capability.log': 'https://vijworkspace17376385887.blob.core.windows.net/azureml/ExperimentRun/dcid.diabetes-experiment_1747400453_7cf23727/system_logs/hosttools_capability/hosttools-capability.log?sv=2019-07-07&sr=b&sig=fg%2BIPUrNloz1aPgEyMJlCMjbjHLab2g3iXPCBUwf5A0%3D&skoid=24a8141d-fcb3-4745-b989-08fa03cec8d2&sktid=d25ccddb-b863-4a84-8923-dbc1177436a0&skt=2025-05-16T00%3A57%3A35Z&ske=2025-05-18T01%3A07%3A35Z&sks=b&skv=2019-07-07&st=2025-05-16T12%3A51%3A19Z&se=2025-05-16T21%3A01%3A19Z&sp=r',\n  'system_logs/lifecycler/execution-wrapper.log': 'https://vijworkspace17376385887.blob.core.windows.net/azureml/ExperimentRun/dcid.diabetes-experiment_1747400453_7cf23727/system_logs/lifecycler/execution-wrapper.log?sv=2019-07-07&sr=b&sig=OF4wL92nzOkR%2BzlJtSIRqJE1xZf%2BTbOp4xzW%2FwqpcdM%3D&skoid=24a8141d-fcb3-4745-b989-08fa03cec8d2&sktid=d25ccddb-b863-4a84-8923-dbc1177436a0&skt=2025-05-16T00%3A57%3A35Z&ske=2025-05-18T01%3A07%3A35Z&sks=b&skv=2019-07-07&st=2025-05-16T12%3A51%3A19Z&se=2025-05-16T21%3A01%3A19Z&sp=r',\n  'system_logs/lifecycler/lifecycler.log': 'https://vijworkspace17376385887.blob.core.windows.net/azureml/ExperimentRun/dcid.diabetes-experiment_1747400453_7cf23727/system_logs/lifecycler/lifecycler.log?sv=2019-07-07&sr=b&sig=YFbQSNjrtb0by1BMYewCAFle00iVDosvZSyRCOugQA4%3D&skoid=24a8141d-fcb3-4745-b989-08fa03cec8d2&sktid=d25ccddb-b863-4a84-8923-dbc1177436a0&skt=2025-05-16T00%3A57%3A35Z&ske=2025-05-18T01%3A07%3A35Z&sks=b&skv=2019-07-07&st=2025-05-16T12%3A51%3A19Z&se=2025-05-16T21%3A01%3A19Z&sp=r',\n  'system_logs/lifecycler/vm-bootstrapper.log': 'https://vijworkspace17376385887.blob.core.windows.net/azureml/ExperimentRun/dcid.diabetes-experiment_1747400453_7cf23727/system_logs/lifecycler/vm-bootstrapper.log?sv=2019-07-07&sr=b&sig=J4a3SWgHGBl17RbANNMxovRg1DBLuslC2mfX4%2BYxH8o%3D&skoid=24a8141d-fcb3-4745-b989-08fa03cec8d2&sktid=d25ccddb-b863-4a84-8923-dbc1177436a0&skt=2025-05-16T00%3A57%3A35Z&ske=2025-05-18T01%3A07%3A35Z&sks=b&skv=2019-07-07&st=2025-05-16T12%3A51%3A19Z&se=2025-05-16T21%3A01%3A19Z&sp=r',\n  'system_logs/metrics_capability/metrics-capability.log': 'https://vijworkspace17376385887.blob.core.windows.net/azureml/ExperimentRun/dcid.diabetes-experiment_1747400453_7cf23727/system_logs/metrics_capability/metrics-capability.log?sv=2019-07-07&sr=b&sig=AMLgj%2BRdtbAYfYbmbfhQ6MF23igntTgC1Y4EzPH2UIc%3D&skoid=24a8141d-fcb3-4745-b989-08fa03cec8d2&sktid=d25ccddb-b863-4a84-8923-dbc1177436a0&skt=2025-05-16T00%3A57%3A35Z&ske=2025-05-18T01%3A07%3A35Z&sks=b&skv=2019-07-07&st=2025-05-16T12%3A51%3A19Z&se=2025-05-16T21%3A01%3A19Z&sp=r',\n  'system_logs/snapshot_capability/snapshot-capability.log': 'https://vijworkspace17376385887.blob.core.windows.net/azureml/ExperimentRun/dcid.diabetes-experiment_1747400453_7cf23727/system_logs/snapshot_capability/snapshot-capability.log?sv=2019-07-07&sr=b&sig=LUZtEkzZGydDWZ1fW%2FXoMaRVhjOlcH6Qq03cO6wnEts%3D&skoid=24a8141d-fcb3-4745-b989-08fa03cec8d2&sktid=d25ccddb-b863-4a84-8923-dbc1177436a0&skt=2025-05-16T00%3A57%3A35Z&ske=2025-05-18T01%3A07%3A35Z&sks=b&skv=2019-07-07&st=2025-05-16T12%3A51%3A19Z&se=2025-05-16T21%3A01%3A19Z&sp=r'},\n 'submittedBy': 'Radha Singh'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1747400481245
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = run.register_model(model_name=\"diabetes-model\", model_path=\"outputs/model.pkl\")\n",
        "print(\"Model Registered Successfully:\", model.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model Registered Successfully: diabetes-model\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1747400573440
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>**VERSION Control**</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>**Better run the following in BASH without ! sign**</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check code quality locally\n",
        "!pip install flake8\n",
        "!flake8 train.py\n",
        "!flake8 score.py\n",
        "\n",
        "# Add code to git repo\n",
        "!git init\n",
        "!git add train.py score.py\n",
        "!git commit -m \"Initial ML scripts\"\n",
        "!git remote add origin <your-repo-url>\n",
        "!git push -u origin main\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: flake8 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (3.8.4)\nRequirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from flake8) (2.2.0)\nRequirement already satisfied: mccabe<0.7.0,>=0.6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from flake8) (0.6.1)\nRequirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from flake8) (2.6.0)\ntrain.py:0:1: E902 FileNotFoundError: [Errno 2] No such file or directory: 'train.py'\nscore.py:0:1: E902 FileNotFoundError: [Errno 2] No such file or directory: 'score.py'\n\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n\u001b[33mhint:\u001b[m\n\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n\u001b[33mhint:\u001b[m\n\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n\u001b[33mhint:\u001b[m\n\u001b[33mhint: \tgit branch -m <name>\u001b[m\nInitialized empty Git repository in /mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj/.git/\nfatal: detected dubious ownership in repository at '/mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj'\nTo add an exception for this directory, call:\n\n\tgit config --global --add safe.directory /mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj\nfatal: detected dubious ownership in repository at '/mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj'\nTo add an exception for this directory, call:\n\n\tgit config --global --add safe.directory /mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj\n/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n/bin/bash: -c: line 1: `git remote add origin <your-repo-url>'\nfatal: detected dubious ownership in repository at '/mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj'\nTo add an exception for this directory, call:\n\n\tgit config --global --add safe.directory /mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1747401033617
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPEN Github make arepository and copy the URL here\n",
        "!git remote add origin https://github.com/vijy24/Daibetes_deployment.git\n",
        "!git push -u origin main\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "fatal: detected dubious ownership in repository at '/mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj'\r\nTo add an exception for this directory, call:\r\n\r\n\tgit config --global --add safe.directory /mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj\r\nfatal: detected dubious ownership in repository at '/mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj'\r\nTo add an exception for this directory, call:\r\n\r\n\tgit config --global --add safe.directory /mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj\r\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1747402505877
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git status\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "fatal: detected dubious ownership in repository at '/mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj'\r\nTo add an exception for this directory, call:\r\n\r\n\tgit config --global --add safe.directory /mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj\r\n"
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global --add safe.directory /mnt/batch/tasks/shared/LS_root/mounts/clusters/vij-compute/code/Users/radha_thaj\n"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1747403334123
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git status\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "On branch master\r\n\r\nNo commits yet\r\n\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n\t\u001b[31m.amlignore\u001b[m\r\n\t\u001b[31m.amlignore.amltmp\u001b[m\r\n\t\u001b[31m.ipynb_aml_checkpoints/\u001b[m\r\n\t\u001b[31mVij-Notebook.ipynb\u001b[m\r\n\t\u001b[31mbatch_predict_async.py\u001b[m\r\n\t\u001b[31mbatch_predict_async.py.amltmp\u001b[m\r\n\t\u001b[31mdiabetes.csv\u001b[m\r\n\t\u001b[31mdiabetes_batch-100.csv\u001b[m\r\n\t\u001b[31mdiabetes_batch-1000.csv\u001b[m\r\n\t\u001b[31mdiabetes_predictions.csv\u001b[m\r\n\t\u001b[31mdiabetes_predictions_async.csv\u001b[m\r\n\t\u001b[31mdiabetes_predictions_async_concurrent.csv\u001b[m\r\n\t\u001b[31mscoring.py\u001b[m\r\n\t\u001b[31mscoring.py.amltmp\u001b[m\r\n\t\u001b[31mtraining.py\u001b[m\r\n\t\u001b[31mtraining.txt.amltmp\u001b[m\r\n\t\u001b[31mvij-notebook.ipynb.amltmp\u001b[m\r\n\r\nnothing added to commit but untracked files present (use \"git add\" to track)\r\n"
        }
      ],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git log\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "fatal: your current branch 'master' does not have any commits yet\r\n"
        }
      ],
      "execution_count": 29,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git add Vij-Notebook.ipynb diabetes.csv\n"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Add notebook and diabetes dataset\"\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Author identity unknown\r\n\r\n*** Please tell me who you are.\r\n\r\nRun\r\n\r\n  git config --global user.email \"you@example.com\"\r\n  git config --global user.name \"Your Name\"\r\n\r\nto set your account's default identity.\r\nOmit --global to set the identity only in this repository.\r\n\r\nfatal: unable to auto-detect email address (got 'azureuser@vij-compute.(none)')\r\n"
        }
      ],
      "execution_count": 31,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"vijy_24@yahoo.in\"\n",
        "!git config --global user.name \"Vijyant\"\n"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Add notebook and diabetes dataset\"\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[master (root-commit) 17e20a4] Add notebook and diabetes dataset\r\n 2 files changed, 1296 insertions(+)\r\n create mode 100644 Vij-Notebook.ipynb\r\n create mode 100644 diabetes.csv\r\n"
        }
      ],
      "execution_count": 34,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git push -u origin master\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "fatal: 'origin' does not appear to be a git repository\r\nfatal: Could not read from remote repository.\r\n\r\nPlease make sure you have the correct access rights\r\nand the repository exists.\r\n"
        }
      ],
      "execution_count": 35,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/vijy24/Daibetes_deployment.git\n"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git push -u origin master\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Username for 'https://github.com': "
        }
      ],
      "execution_count": 37,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git ls-files\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Vij-Notebook.ipynb\r\ndiabetes.csv\r\n"
        }
      ],
      "execution_count": 39,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark>## **DEPLOYMENT**</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import Model\n",
        "from azureml.core.webservice import AciWebservice, Webservice\n",
        "from azureml.core.model import InferenceConfig\n",
        "\n",
        "inference_config = InferenceConfig(entry_script=\"scoring.py\", environment=env)\n",
        "\n",
        "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
        "\n",
        "service = Model.deploy(ws, \"diabetes-service\", [model], inference_config, deployment_config)\n",
        "service.wait_for_deployment(show_output=True)\n",
        "\n",
        "print(\"Service Deployed at:\", service.scoring_uri)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_6833/645971661.py:9: FutureWarning: azureml.core.model:\nTo leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \nplease refer to respective documentations \nhttps://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\nhttps://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \nFor more information on migration, see https://aka.ms/acimoemigration \nTo disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n  service = Model.deploy(ws, \"diabetes-service\", [model], inference_config, deployment_config)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\nRunning\n2025-05-16 12:18:52+00:00 Creating Container Registry if not exists.\n2025-05-16 12:18:52+00:00 Registering the environment.\n2025-05-16 12:18:52+00:00 Use the existing image.\n2025-05-16 12:18:52+00:00 Generating deployment configuration.\n2025-05-16 12:18:54+00:00 Submitting deployment to compute.\n2025-05-16 12:19:01+00:00 Checking the status of deployment diabetes-service..\n2025-05-16 12:20:15+00:00 Checking the status of inference endpoint diabetes-service.\nSucceeded\nACI service creation operation finished, operation \"Succeeded\"\nService Deployed at: http://d52f27c9-d0b2-4c55-94bf-e3a8109e67c6.uksouth.azurecontainer.io/score\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1747398023615
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# Example input ‚Äî replace with real feature values from your dataset\n",
        "sample_input = {\n",
        "    \"data\": [[0.038, 0.05, 0.061, 0.021, 0.046, 0.033, 0.026, 0.008, 0.012, 0.019]]\n",
        "}\n",
        "\n",
        "input_json = json.dumps(sample_input)\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1747398100442
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual scoring URI\n",
        "scoring_uri = service.scoring_uri\n",
        "\n",
        "# Only needed if authentication is enabled\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "# Make the request\n",
        "response = requests.post(scoring_uri, data=input_json, headers=headers)\n",
        "\n",
        "print(\"Prediction:\", response.json())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Prediction: [145.91014672025526]\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1747398103728
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(service.scoring_uri)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "http://d52f27c9-d0b2-4c55-94bf-e3a8109e67c6.uksouth.azurecontainer.io/score\n"
        }
      ],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1747477241912
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "# Step 1: Load your CSV file\n",
        "df = pd.read_csv(\"diabetes_batch.csv\")  # üëà replace with your actual file\n",
        "print(f\"Loaded {len(df)} rows\")\n",
        "\n",
        "# Step 2: Define scoring URI and headers\n",
        "scoring_uri = \"http://d52f27c9-d0b2-4c55-94bf-e3a8109e67c6.uksouth.azurecontainer.io/score\"  # üëà replace with your actual URI\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "# Step 3: Send each row for prediction and collect results\n",
        "predictions = []\n",
        "\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    data = {\"data\": [row.tolist()]}\n",
        "    input_json = json.dumps(data)\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(scoring_uri, data=input_json, headers=headers)\n",
        "        prediction = response.json()\n",
        "    except Exception as e:\n",
        "        prediction = [str(e)]\n",
        "    \n",
        "    predictions.append(prediction[0])  # Assuming a single prediction\n",
        "\n",
        "# Step 4: Save predictions to new CSV\n",
        "df[\"prediction\"] = predictions\n",
        "df.to_csv(\"diabetes_predictions.csv\", index=False)\n",
        "print(\"‚úÖ Predictions saved to diabetes_predictions.csv\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1747346681374
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: aiohttp in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (3.11.16)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (1.3.2)\r\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (5.0.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (6.4.3)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (1.5.0)\r\nRequirement already satisfied: propcache>=0.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (0.3.1)\r\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (25.3.0)\r\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (1.19.0)\r\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (2.6.1)\r\nRequirement already satisfied: typing-extensions>=4.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp) (4.13.2)\r\nRequirement already satisfied: idna>=2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\r\n"
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp\n",
        "!pip show aiohttp\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: aiohttp in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (3.11.16)\nRequirement already satisfied: propcache>=0.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (0.3.1)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (5.0.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (1.3.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (6.4.3)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (1.19.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (1.5.0)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from aiohttp) (25.3.0)\nRequirement already satisfied: typing-extensions>=4.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp) (4.13.2)\nRequirement already satisfied: idna>=2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\nName: aiohttp\nVersion: 3.11.16\nSummary: Async http client/server framework (asyncio)\nHome-page: https://github.com/aio-libs/aiohttp\nAuthor: \nAuthor-email: \nLicense: Apache-2.0\nLocation: /anaconda/envs/azureml_py38/lib/python3.10/site-packages\nRequires: aiohappyeyeballs, aiosignal, async-timeout, attrs, frozenlist, multidict, propcache, yarl\nRequired-by: adlfs, aiohttp-cors, datasets\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1747347519849
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.executable)  # Confirm path is /anaconda/envs/azureml_py310_sdkv2/bin/python\n",
        "!{sys.executable} -m pip install aiohttp\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/anaconda/envs/azureml_py310_sdkv2/bin/python\nCollecting aiohttp\n  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp) (1.3.2)\nCollecting async-timeout<6.0,>=4.0 (from aiohttp)\n  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp) (1.6.0)\nCollecting multidict<7.0,>=4.5 (from aiohttp)\n  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp)\n  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp)\n  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\nRequirement already satisfied: typing-extensions>=4.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp) (4.13.2)\nRequirement already satisfied: idna>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\nDownloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nDownloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\nDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\nDownloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\nInstalling collected packages: propcache, multidict, async-timeout, aiohappyeyeballs, yarl, aiohttp\nSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 async-timeout-5.0.1 multidict-6.4.3 propcache-0.3.1 yarl-1.20.0\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1747347688575
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "print(aiohttp.__version__)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3.11.18\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1747347813378
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show aiohttp"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name: aiohttp\r\nVersion: 3.11.16\r\nSummary: Async http client/server framework (asyncio)\r\nHome-page: https://github.com/aio-libs/aiohttp\r\nAuthor: \r\nAuthor-email: \r\nLicense: Apache-2.0\r\nLocation: /anaconda/envs/azureml_py38/lib/python3.10/site-packages\r\nRequires: aiohappyeyeballs, aiosignal, async-timeout, attrs, frozenlist, multidict, propcache, yarl\r\nRequired-by: adlfs, aiohttp-cors, datasets\r\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/anaconda/envs/azureml_py310_sdkv2/bin/python\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1747347829656
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: nest_asyncio in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.6.0)\r\n"
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "nest_asyncio.apply()\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1747348070196
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Rows in DataFrame: {len(df)}\")\n",
        "print(f\"Length of predictions: {len(predictions)}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Rows in DataFrame: 1024\nLength of predictions: 1627\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1747348247792
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <mark>** Sequential Async Batch Prediction Script**</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import aiohttp\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "# Step 2: Apply nest_asyncio so async works in Jupyter/interactive notebooks\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Load and clean your data\n",
        "df = pd.read_csv(\"diabetes_batch-1000.csv\", header=None)   # Update with your filename if needed\n",
        "df = df.dropna()  # Remove any rows with all NaN values (often blank rows at end)\n",
        "print(f\"[INFO] Cleaned DataFrame rows: {len(df)}\")\n",
        "\n",
        "# Step 4: Set up endpoint and batch config\n",
        "scoring_uri = \"http://4d19bf24-c355-4b9e-87ed-dcf77f3deda1.uksouth.azurecontainer.io/score\"   # <-- Put your endpoint here!\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "batch_size = 50  # Number of rows per batch; adjust as needed\n",
        "\n",
        "# Step 5: Define the async batch sending function\n",
        "async def send_batch(session, batch_data, batch_num):\n",
        "    \"\"\"\n",
        "    Sends a batch of data for prediction.\n",
        "    \"\"\"\n",
        "    payload = {\"data\": batch_data}\n",
        "    try:\n",
        "        async with session.post(scoring_uri, json=payload) as resp:\n",
        "            result = await resp.json()\n",
        "            # Log batch info\n",
        "            print(f\"[BATCH {batch_num}] Sent {len(batch_data)} rows, received {len(result)} predictions\")\n",
        "            return result  # Should be a list of predictions for the batch\n",
        "    except Exception as e:\n",
        "        print(f\"[BATCH {batch_num}] ERROR: {e}\")\n",
        "        return [str(e)] * len(batch_data)  # Fill this batch with error message(s)\n",
        "\n",
        "# Step 6: Define the main async batching runner\n",
        "async def run_async_batch_predictions(df, batch_size):\n",
        "    \"\"\"\n",
        "    Runs prediction for the DataFrame in batches asynchronously.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    async with aiohttp.ClientSession(headers=headers) as session:\n",
        "        num_batches = (len(df) + batch_size - 1) // batch_size  # Calculate number of batches\n",
        "        for batch_num, start in enumerate(range(0, len(df), batch_size), 1):\n",
        "            end = min(start + batch_size, len(df))\n",
        "            batch_data = df.iloc[start:end].values.tolist()\n",
        "            # Send this batch for prediction\n",
        "            batch_predictions = await send_batch(session, batch_data, batch_num)\n",
        "            # Defensive: make sure batch_predictions is a list\n",
        "            if not isinstance(batch_predictions, list):\n",
        "                batch_predictions = [batch_predictions]\n",
        "            predictions.extend(batch_predictions)\n",
        "            print(f\"[INFO] Accumulated {len(predictions)} predictions so far\")\n",
        "    return predictions\n",
        "\n",
        "# Step 7: Run the async batch prediction and save results\n",
        "predictions = await run_async_batch_predictions(df, batch_size)\n",
        "print(f\"[FINAL] Rows in DataFrame: {len(df)}; Total predictions: {len(predictions)}\")\n",
        "\n",
        "# Step 8: Attach predictions to the DataFrame and save to CSV\n",
        "df[\"prediction\"] = predictions\n",
        "df.to_csv(\"diabetes_predictions_async_batch.csv\", index=False)\n",
        "print(\"‚úÖ [DONE] Saved predictions to diabetes_predictions_async_batch.csv\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[INFO] Cleaned DataFrame rows: 1000\n[BATCH 1] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 50 predictions so far\n[BATCH 2] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 100 predictions so far\n[BATCH 3] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 150 predictions so far\n[BATCH 4] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 200 predictions so far\n[BATCH 5] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 250 predictions so far\n[BATCH 6] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 300 predictions so far\n[BATCH 7] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 350 predictions so far\n[BATCH 8] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 400 predictions so far\n[BATCH 9] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 450 predictions so far\n[BATCH 10] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 500 predictions so far\n[BATCH 11] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 550 predictions so far\n[BATCH 12] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 600 predictions so far\n[BATCH 13] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 650 predictions so far\n[BATCH 14] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 700 predictions so far\n[BATCH 15] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 750 predictions so far\n[BATCH 16] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 800 predictions so far\n[BATCH 17] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 850 predictions so far\n[BATCH 18] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 900 predictions so far\n[BATCH 19] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 950 predictions so far\n[BATCH 20] Sent 50 rows, received 50 predictions\n[INFO] Accumulated 1000 predictions so far\n[FINAL] Rows in DataFrame: 1000; Total predictions: 1000\n‚úÖ [DONE] Saved predictions to diabetes_predictions_async_batch.csv\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1747350076243
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <mark>** Concurrent Async Batch Prediction Script**</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "import aiohttp\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "# Step 2: Enable nested asyncio for Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Load and clean your data\n",
        "df = pd.read_csv(\"diabetes_batch-100.csv\", header=None)\n",
        "df = df.dropna()\n",
        "print(f\"[INFO] Cleaned DataFrame rows: {len(df)}\")  # Should print 100\n",
        "\n",
        "# Step 4: Set up endpoint and config\n",
        "scoring_uri = \"http://4d19bf24-c355-4b9e-87ed-dcf77f3deda1.uksouth.azurecontainer.io/score\"  # <- Replace with your endpoint!\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "batch_size = 20\n",
        "\n",
        "# Step 5: Define async function to send a batch\n",
        "async def send_batch(session, batch_data, batch_num):\n",
        "    payload = {\"data\": batch_data}\n",
        "    try:\n",
        "        async with session.post(scoring_uri, json=payload) as resp:\n",
        "            result = await resp.json()\n",
        "            print(f\"[BATCH {batch_num}] Sent {len(batch_data)} rows, received {len(result)} predictions\")\n",
        "            return result\n",
        "    except Exception as e:\n",
        "        print(f\"[BATCH {batch_num}] ERROR: {e}\")\n",
        "        return [str(e)] * len(batch_data)\n",
        "\n",
        "# Step 6: Define main concurrent async runner (all batches at once)\n",
        "async def run_concurrent_batches(df, batch_size):\n",
        "    async with aiohttp.ClientSession(headers=headers) as session:\n",
        "        tasks = []\n",
        "        for batch_num, start in enumerate(range(0, len(df), batch_size), 1):\n",
        "            end = min(start + batch_size, len(df))\n",
        "            batch_data = df.iloc[start:end].values.tolist()\n",
        "            tasks.append(send_batch(session, batch_data, batch_num))\n",
        "        # Run all batch requests concurrently!\n",
        "        all_results = await asyncio.gather(*tasks)\n",
        "        # Flatten the results: all_results is a list of lists (one per batch)\n",
        "        predictions = [pred for batch in all_results for pred in batch]\n",
        "        return predictions\n",
        "\n",
        "# Step 7: Run concurrent async batch prediction and save results\n",
        "predictions = await run_concurrent_batches(df, batch_size)\n",
        "print(f\"[FINAL] Rows in DataFrame: {len(df)}; Total predictions: {len(predictions)}\")\n",
        "df[\"prediction\"] = predictions\n",
        "df.to_csv(\"diabetes_predictions_async_concurrent.csv\", index=False)\n",
        "print(\"‚úÖ [DONE] Saved predictions to diabetes_predictions_async_concurrent.csv\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[INFO] Cleaned DataFrame rows: 100\n[BATCH 5] Sent 20 rows, received 20 predictions\n[BATCH 4] Sent 20 rows, received 20 predictions\n[BATCH 3] Sent 20 rows, received 20 predictions\n[BATCH 2] Sent 20 rows, received 20 predictions\n[BATCH 1] Sent 20 rows, received 20 predictions\n[FINAL] Rows in DataFrame: 100; Total predictions: 100\n‚úÖ [DONE] Saved predictions to diabetes_predictions_async_concurrent.csv\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1747353287732
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <mark>**Concurrent process Thread based **</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "# Load your data\n",
        "df = pd.read_csv(\"diabetes_batch-100.csv\", header=None)\n",
        "scoring_uri = \"http://4d19bf24-c355-4b9e-87ed-dcf77f3deda1.uksouth.azurecontainer.io/score\"\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "# Function to send one row\n",
        "def predict_single_row(row):\n",
        "    data = {\"data\": [row.tolist()]}\n",
        "    try:\n",
        "        response = requests.post(scoring_uri, data=json.dumps(data), headers=headers)\n",
        "        return response.json()[0]\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "# Run predictions asynchronously\n",
        "predictions = []\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:  # üîÅ change 10 to control parallelism\n",
        "    futures = [executor.submit(predict_single_row, row) for _, row in df.iterrows()]\n",
        "    for future in as_completed(futures):\n",
        "        predictions.append(future.result())\n",
        "\n",
        "# Attach results and save\n",
        "df[\"prediction\"] = predictions\n",
        "df.to_csv(\"diabetes_predictions_async.csv\", index=False)\n",
        "print(\"‚úÖ Saved predictions to diabetes_predictions_async.csv\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "‚úÖ Saved predictions to diabetes_predictions_async.csv\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1747353959502
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <mark>## **AIOHTTP + ASYNC -Concurrent Batch Prediction Pipeline -Schedule a Job to run every Monday and Thursday**</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Register the Script in a Pipeline Step (Jupyter Notebook)**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "print(azureml.core.VERSION)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1.60.0\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1747358616694
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.executable)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/anaconda/envs/azureml_py310_sdkv2/bin/python\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1747357086830
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m pip show azureml-core\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name: azureml-core\r\nVersion: 1.60.0\r\nSummary: Azure Machine Learning core packages, modules, and classes\r\nHome-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py\r\nAuthor: Microsoft Corp\r\nAuthor-email: \r\nLicense: https://aka.ms/azureml-sdk-license\r\nLocation: /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages\r\nRequires: adal, argcomplete, azure-common, azure-core, azure-graphrbac, azure-mgmt-authorization, azure-mgmt-containerregistry, azure-mgmt-keyvault, azure-mgmt-network, azure-mgmt-resource, azure-mgmt-storage, backports.tempfile, contextlib2, docker, humanfriendly, jmespath, jsonpickle, knack, msal, msal-extensions, msrest, msrestazure, ndg-httpsclient, packaging, paramiko, pathspec, pkginfo, PyJWT, pyopenssl, python-dateutil, pytz, requests, SecretStorage, urllib3\r\nRequired-by: azureml-pipeline-core, azureml-sdk, azureml-telemetry, azureml-train-automl-client, azureml-train-core\r\n"
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "env = Environment(\"test-env\")\n",
        "env.python.conda_dependencies.add_pip_package(\"pandas\")\n",
        "\n",
        "run_config = RunConfiguration()\n",
        "run_config.environment = env\n",
        "\n",
        "step = PythonScriptStep(\n",
        "    name=\"TestStep\",\n",
        "    script_name=\"test.py\",\n",
        "    source_directory=\".\",\n",
        "    compute_target=\"cpu-cluster\",  # or any compute name you have\n",
        "    runconfig=run_config,\n",
        "    allow_reuse=False\n",
        ")\n",
        "print(\"‚úÖ PythonScriptStep created successfully!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "‚úÖ PythonScriptStep created successfully!\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1747358913144
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "env = Environment(\"async-batch-env\")\n",
        "env.python.conda_dependencies.add_pip_package(\"pandas\")\n",
        "env.python.conda_dependencies.add_pip_package(\"aiohttp\")\n",
        "env.python.conda_dependencies.add_pip_package(\"nest_asyncio\")\n",
        "\n",
        "run_config = RunConfiguration()\n",
        "run_config.environment = env\n"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1747358920894
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "step = PythonScriptStep(\n",
        "    name=\"AsyncBatchPredictionStep\",\n",
        "    script_name=\"batch_predict_async.py\",\n",
        "    source_directory=\".\",  # Your script's folder\n",
        "    compute_target=compute_target,  # Already defined in your notebook\n",
        "    runconfig=run_config,\n",
        "    allow_reuse=False\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1747358926180
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Build and Submit the Pipeline**\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "dataset = Dataset.File.from_files(path=(datastore, 'mydata/diabetes_batch-100.csv'))\n",
        "dataset = dataset.register(\n",
        "    workspace=ws,\n",
        "    name=\"diabetes-batch-input\",\n",
        "    create_new_version=True\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1747359867787
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.core import Experiment\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[step])\n",
        "pipeline.validate()\n",
        "pipeline_run = Experiment(ws, \"async-batch-prediction-pipeline\").submit(pipeline)\n",
        "print(\"Pipeline submitted. Run ID:\", pipeline_run.id)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step AsyncBatchPredictionStep is ready to be created [91a42fc9]\nCreated step AsyncBatchPredictionStep [91a42fc9][b682c9b8-7039-42db-a82c-8b4c772075db], (This step will run and generate new outputs)\nSubmitted PipelineRun 046d04fd-0289-47be-9823-41dd275dfbbd\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/046d04fd-0289-47be-9823-41dd275dfbbd?wsid=/subscriptions/1a1d0c91-b4c7-49a1-a6ef-2cea4ddaadb3/resourcegroups/vij-resource-group-1/workspaces/vij-workspace-1&tid=d25ccddb-b863-4a84-8923-dbc1177436a0\nPipeline submitted. Run ID: 046d04fd-0289-47be-9823-41dd275dfbbd\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1747358933115
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schedule the Pipeline for Monday and Thursday**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline = pipeline.publish(\n",
        "    name=\"async-batch-prediction-pipeline\",\n",
        "    description=\"Batch prediction pipeline (async batch aiohttp)\",\n",
        "    version=\"1.0\"\n",
        ")\n",
        "print(\"Published pipeline ID:\", published_pipeline.id)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Published pipeline ID: 1f92b03f-c6e2-44e2-8c42-8476038512aa\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1747359013171
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = Schedule.create(\n",
        "    workspace=ws,\n",
        "    name=\"mon-thu-batch-prediction\",\n",
        "    pipeline_id=published_pipeline.id,  # Use the published pipeline's id!\n",
        "    experiment_name=\"async-batch-prediction-pipeline\",\n",
        "    recurrence=recurrence,\n",
        "    description=\"Run batch prediction every Monday and Thursday\",\n",
        "    wait_for_provisioning=True\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "step = PythonScriptStep(\n",
        "    name=\"AsyncBatchPredictionStep\",\n",
        "    script_name=\"batch_predict_async.py\",\n",
        "    source_directory=\".\",\n",
        "    compute_target=compute_target,\n",
        "    runconfig=run_config,\n",
        "    arguments=[\"--input_data\", \"dummy\"],  # <-- Add this line\n",
        "    allow_reuse=False\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1747359347899
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}